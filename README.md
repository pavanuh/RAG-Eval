
# Retrieval-Augmented Generation (RAG) Framework Implementation

This repository contains the implementations of various Retrieval-Augmented Generation (RAG) architectures, including Base RAG, Corrective RAG, RAG Fusion, Self-RAG, and Agentic RAG. Each architecture has been designed and implemented to explore how RAG frameworks can improve large language model (LLM) performance in retrieving and generating contextually relevant responses.

## Project Overview

Retrieval-Augmented Generation (RAG) improves large language models by integrating a retriever (for fetching external knowledge) with a generator (for creating natural language responses). This project explores different RAG variants, evaluates their performance using advanced metrics, and highlights their applications in diverse domains, such as education, customer service, and healthcare.

### Key Features
- **Base RAG:** Combines retrieval and generation for contextual question-answering.
- **Corrective RAG:** Adds feedback loops to refine responses.
- **RAG Fusion:** Integrates multiple retrieval strategies for enhanced context coverage.
- **Self-RAG:** Introduces iterative feedback and self-enhancement mechanisms.
- **Agentic RAG:** Includes decision-making capabilities for dynamic query handling.

### File Descriptions
- `agentic_rag.py`: Implementation of Agentic RAG.
- `base_rag.py`: Basic RAG architecture.
- `corr_rag.py`: Corrective RAG implementation.
- `rag_fusion.py`: Implementation of RAG Fusion.
- `rag_eval.py`: Evaluation script for all RAG models.
- `self_rag.py`: Implementation of Self-RAG.

---

## Getting Started

### Prerequisites
- Python 3.9 or above
- Required Python libraries:
  - `faiss-cpu`
  - `langchain`
  - `pandas`
  - `openai`
  - `ragas`

### Dataset
The dataset used for this project is available on [Kaggle](https://www.kaggle.com/datasets/samuelmatsuoharris/single-topic-rag-evaluation-dataset). It includes single-topic documents for evaluating Retrieval-Augmented Generation (RAG) models. The dataset is structured to test retrieval and generation performance across three categories:

1. **Unanswerable Questions**: Questions without answers in the documents.
2. **Single-Text Retrieval**: Questions whose answers are found in a single document.
3. **Multi-Text Retrieval**: Questions requiring synthesized answers from multiple documents.

#### Setup
Download the dataset and save it in the `data/` directory within the project folder. Ensure the files are named consistently with the paths specified in the scripts.

---

## Running the Scripts
Each script is designed to run independently and implements a specific RAG architecture. Follow the instructions below to execute them:

1. **Base RAG**
   Run the script to execute the basic RAG framework.
   ```bash
   python base_rag.py
   ```

   **Key Features:**
   - Combines retrieval and generation for contextual question-answering.
   - Modular and straightforward for basic retrieval-augmented tasks.

2. **Corrective RAG**
   Corrective RAG builds upon the Basic RAG framework by introducing a feedback loop that evaluates and refines the initial responses generated by the model. This script is particularly useful for ambiguous or partially defined queries, as it enhances response accuracy through iterative refinement.
   ```bash
   python corr_rag.py
   ```

   **Key Features:**
   - Feedback loop for error correction and response enhancement.
   - Suitable for tasks requiring high accuracy and detailed answers.
   - May be slower than Basic RAG due to the iterative feedback mechanism.

3. **RAG Fusion**
   RAG Fusion combines multiple retrieval strategies to maximize context coverage and reduce the limitations of single-retrieval methods. This script is ideal for handling diverse queries requiring information from multiple contexts.
   ```bash
   python rag_fusion.py
   ```

   **Key Features:**
   - Concurrent retrieval using various chunking configurations.
   - Aggregates results from multiple retrievers for unified input.
   - Suitable for complex queries requiring broader contextual inputs.

4. **Self-RAG**
   Self-RAG extends Corrective RAG by incorporating additional iterative feedback loops and self-assessment mechanisms. This script continuously improves response quality by monitoring and revising its outputs.
   ```bash
   python self_rag.py
   ```

   **Key Features:**
   - Employs multiple retrievers for comprehensive contextual coverage.
   - Uses iterative feedback loops to refine accuracy and flexibility.
   - Particularly suitable for applications demanding high precision.

5. **Agentic RAG**
   Agentic RAG introduces decision-making capabilities, allowing the model to dynamically adjust its retrieval strategies based on query-specific requirements. This architecture is ideal for task-oriented applications requiring adaptive behavior.
   ```bash
   python agentic_rag.py
   ```

   **Key Features:**
   - Zero-shot learning for task-specific responses without additional training.
   - Adaptive retrieval strategies for handling dynamic queries.
   - Designed for fast and complex scenarios.

6. **Evaluation**
   The evaluation script compares the performance of all RAG models using various similarity metrics such as ROUGE, Levenshtein Distance, and Hamming Distance.
   ```bash
   python rag_eval.py
   ```

   **Key Features:**
   - Generates detailed performance reports for all RAG models.
   - Stores evaluation results in the `results/` directory.
   - Facilitates comparative analysis of model accuracy and relevance.

---

## Output
Each script generates output files containing:
- Retrieved passages from the dataset.
- Generated answers for the input queries.
- Evaluation metrics (e.g., similarity scores, accuracy).

The outputs are stored in the `results/` directory for easy access and analysis.

---

### Example Usage
To run RAG Fusion and evaluate its performance:
1. Execute the RAG Fusion script:
   ```bash
   python rag_fusion.py
   ```
2. Run the evaluation script:
   ```bash
   python rag_eval.py
   ```
3. Review the results in the `results/` directory.

---

### Notes
- Ensure the dataset is correctly placed in the `data/` directory before running the scripts.
- Adjust configurations (e.g., chunk sizes, retrieval strategies) in the scripts as needed to match your use case.
